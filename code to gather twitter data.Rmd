---
title: "getting the twitter data"
authors: "Sammy Salustri and Yagmur Idil Ozdemir"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(twitteR)
library(tm)
library(readxl)
library(dplyr)
```

##Get Access to twitter Data: We created credentials for "Politicians Language Analysis." App ID is 15958126.  

```{r}

consumer_key <- ""
consumer_secret<- ""
access_token <- ""
access_secret <- ""
#github got mad at me for posting these when I pushed this file. I have the actual numbers on my computer.

setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)
```

## Create dataset containing all tweets of all politicians and the times they were created. For preliminary testing to make sure all twitter accounts are accessible and for run-time concerns, we only take in one tweet per politician. We will change this. 

```{r }

politicians_df<-read_xlsx("politicians.xlsx")
handles<-politicians_df$Twitter

get_tweets<- function(user) {
  user_data<-userTimeline(user, n=1, includeRts=FALSE)
  result_df<- twListToDF(user_data) %>% select(text, created,screenName)
  return(result_df)
}

all_data<-rbind(lapply(handles, FUN=get_tweets))
results<-do.call("rbind", all_data)

```

## Return the frequency of the words that are parsed from each tweet, returned as a list. We clean the data from any punctuation, turn words to lower case and exclude stopwords. 

```{r}
#split_data <- strsplit(user_data$text, split=" ")
freq <- termFreq(results$text, control = list(removePunctuation = TRUE, tolower = TRUE, stopwords = TRUE))

```

